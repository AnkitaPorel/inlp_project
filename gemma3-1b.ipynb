{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jUe_NE4EUOx"
      },
      "outputs": [],
      "source": [
        "#@title Colab Extra Install { display-mode: \"form\" }\n",
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    !pip install --no-deps unsloth vllm\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    # Skip restarting message in Colab\n",
        "    import sys, re, requests; modules = list(sys.modules.keys())\n",
        "    for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft \"trl==0.15.2\" triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "\n",
        "    # vLLM requirements - vLLM breaks Colab due to reinstalling numpy\n",
        "    f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
        "    with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
        "        file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
        "    !pip install -r vllm_requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title For environments other than Google Colab\n",
        "%pip install unsloth vllm"
      ],
      "metadata": {
        "id": "3vhQ3ehvEXzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title For Loading Checkpoints from Google Drive (on colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sNlwd6zpEaSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title For setting up dataset in Kaggle\n",
        "!git clone https://github.com/shuhanmirza/Bengali-Poem-Dataset.git\n",
        "%cd /kaggle/working/Bengali-Poem-Dataset"
      ],
      "metadata": {
        "id": "oSCT2H8OEc2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title For setting up dataset in Kaggle\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "dataset_dir = 'dataset'\n",
        "\n",
        "data = []\n",
        "\n",
        "for poet in os.listdir(dataset_dir):\n",
        "    poet_dir = os.path.join(dataset_dir, poet)\n",
        "    if os.path.isdir(poet_dir):\n",
        "        for poem in os.listdir(poet_dir):\n",
        "            poem_dir = os.path.join(poet_dir, poem)\n",
        "            if os.path.isdir(poem_dir):\n",
        "                class_text = None\n",
        "                class_file_path = os.path.join(poem_dir, 'CLASS.txt')\n",
        "                if os.path.exists(class_file_path):\n",
        "                    with open(class_file_path, 'r', encoding='utf-8') as class_file:\n",
        "                        class_text = class_file.read().strip()\n",
        "\n",
        "                for file in os.listdir(poem_dir):\n",
        "                    if file.endswith('.txt') and file not in ['CLASS.txt', 'SOURCE.txt']:\n",
        "                        file_path = os.path.join(poem_dir, file)\n",
        "                        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                            poem_text = f.read()\n",
        "                            title = os.path.splitext(file)[0]\n",
        "                            data.append({\n",
        "                                'poet': poet,\n",
        "                                'category': class_text,\n",
        "                                'title': title,\n",
        "                                'poem': poem_text\n",
        "                            })\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df.to_csv('poems_dataset.csv', index=False, encoding='utf-8')\n",
        "!mv /kaggle/working/Bengali-Poem-Dataset/poems_dataset.csv /kaggle/working/\n",
        "%cd /kaggle/working/\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "s1id0sBaEglT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Loading the last checkpoint from Google drive\n",
        "!cp /content/drive/MyDrive/adapter_model.safetensors /content/outputs/checkpoint-1500/"
      ],
      "metadata": {
        "id": "Is6G10iwEpDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Loading the Gemma3 1 billion Parameters using unsloth\n",
        "from unsloth import FastModel\n",
        "import torch\n",
        "\n",
        "fourbit_models = [\n",
        "    # 4bit dynamic quants for superior accuracy and low memory use\n",
        "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
        "\n",
        "    # Other popular models!\n",
        "    \"unsloth/Llama-3.1-8B\",\n",
        "    \"unsloth/Llama-3.2-3B\",\n",
        "    \"unsloth/Llama-3.3-70B\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
        "    \"unsloth/Phi-4\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = \"unsloth/gemma-3-1b-it\",\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True,\n",
        "    load_in_8bit = False,\n",
        "    full_finetuning = False,\n",
        "    # token = \"hf_...\", # use one if using gated models\n",
        ")"
      ],
      "metadata": {
        "id": "EGIFplaZEqLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title To apply Parameter-Efficient Fine-Tuning (PEFT) to the pre-trained Gemma-3-4b model (unsloth provided)\n",
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    r = 64,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"do\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = True,\n",
        "    loftq_config = None,\n",
        ")"
      ],
      "metadata": {
        "id": "jvjad8Y1Et1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Prompt and Dataset Loading (upload the poems_dataset.csv) to Google colab\n",
        "prompt = \"\"\"You are tasked with writing a poem related to the title in the style mentioned below. The poem should fit the specified category.\n",
        "\n",
        "### Title:\n",
        "{}\n",
        "### Category:\n",
        "{}\n",
        "\n",
        "### Poem:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatting_prompts_func(examples):\n",
        "    titles    = examples[\"title\"]\n",
        "    categorys = examples[\"category\"]\n",
        "    poems     = examples[\"poem\"]\n",
        "    texts = []\n",
        "    for title, category, poem in zip(titles, categorys, poems):\n",
        "        text = prompt.format(title, category, poem) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"csv\", data_files=\"/content/poems_dataset.csv\", split=\"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ],
      "metadata": {
        "id": "7qNlaCsIEwC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Set up a training pipeline using the UnslothTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
        "\n",
        "max_seq_length = 2048\n",
        "\n",
        "trainer = UnslothTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "\n",
        "    args = UnslothTrainingArguments(\n",
        "        report_to = \"wandb\",\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_ratio = 0.1,\n",
        "        num_train_epochs = 3,\n",
        "        learning_rate = 5e-5,\n",
        "        embedding_learning_rate = 1e-5,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"cosine\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "3JO2Q6mmE4zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Start training. Use trainer.train() to start from Zero\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "JrFazwLIE7CG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title wandb login setup (needs your API key)\n",
        "!wandb login\n",
        "import wandb\n",
        "wandb.init()"
      ],
      "metadata": {
        "id": "5v1fS7GUE_nK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"gemma-poetry-bn\")\n",
        "tokenizer.save_pretrained(\"gemma-poetry-bn\")"
      ],
      "metadata": {
        "id": "WIhuGVOANsN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Huggingface login\n",
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "id": "z3rsibYUNtkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Publish to Huggingface\n",
        "! huggingface-cli upload Ankita-Porel/gemma3-1b-v1 gemma-poetry-bn"
      ],
      "metadata": {
        "id": "3oc-YjivNv73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load the model from huggingface for inference\n",
        "from unsloth import FastModel\n",
        "import torch\n",
        "\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name=\"Ankita-Porel/gemma3-1b-v1\",\n",
        "    max_seq_length=2048,\n",
        "    load_in_4bit=True,\n",
        "    load_in_8bit = False,\n",
        "    full_finetuning = False,\n",
        "    # token=\"hf_...\"\n",
        ")"
      ],
      "metadata": {
        "id": "uBzcauQOFFK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run the tests\n",
        "model = FastModel.for_inference(model)\n",
        "\n",
        "prompt = \"\"\"You are tasked with writing a poem related to the title in the style mentioned below. The poem should fit the specified category.\n",
        "\n",
        "### Title:\n",
        "{}\n",
        "### Category:\n",
        "{}\n",
        "\n",
        "### Poem:\n",
        "{}\"\"\"\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    prompt.format(\n",
        "        \"বাংলার বায়ু, বাংলার ফল- পূণ্য হউক, পূণ্য হউক,\",\n",
        "        \"কীর্তন\",\n",
        "        \"\",\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 64, repetition_penalty = 2.0, use_cache = True)\n",
        "\n",
        "generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "with open(\"generated_output.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(generated_text)\n",
        "\n",
        "print(\"Output saved to 'output.txt'\")\n",
        "tokenizer.batch_decode(outputs)"
      ],
      "metadata": {
        "id": "UoacBWuXFHq5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}